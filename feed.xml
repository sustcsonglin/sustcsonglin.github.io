<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sustcsonglin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sustcsonglin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-21T01:02:52+00:00</updated><id>https://sustcsonglin.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DeltaNet Explained (Part I)</title><link href="https://sustcsonglin.github.io/blog/2024/deltanet-1/" rel="alternate" type="text/html" title="DeltaNet Explained (Part I)"/><published>2024-12-03T22:25:00+00:00</published><updated>2024-12-03T22:25:00+00:00</updated><id>https://sustcsonglin.github.io/blog/2024/deltanet-1</id><content type="html" xml:base="https://sustcsonglin.github.io/blog/2024/deltanet-1/"><![CDATA[<p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a> (w/ <a href="https://berlino.github.io/">Bailin Wang</a>, <a href="https://yzhang.site/">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/">Yoon Kim</a>). You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf">here</a>.</strong></p> <ol> <li><a href="#">Part I - The Model</a></li> <li><a href="/blog/2024/deltanet-2/">Part II - The Algorithm</a></li> <li><a href="/blog/2024/deltanet-3/">Part III - The Neural Architecture</a></li> </ol> <h2 id="linear-attention-as-rnn">Linear attention as RNN</h2> <p>Notations: we use CAPITAL BOLD letters to represent matrices, lowercase bold letters to represent vectors, and regular lowercase letters to represent scalars.</p> <h3 id="what-is-linear-attention">What is linear attention?</h3> <p>The vanilla softmax attention mechanism, though powerful, suffers from quadratic complexity in sequence length. Let’s see how linear attention addresses this issue by starting with the standard softmax attention (assuming single head):</p> \[\begin{aligned} \mathrm{Parallel\ training:} &amp;&amp;&amp; \mathbf{O} = \mathrm{softmax}(\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M})\mathbf{V} &amp;&amp;\in \mathbb{R}^{L\times d} \\ \mathrm{Iterative\ inference:} &amp;&amp;&amp;\mathbf{o_t} = \sum_{j=1}^t \frac{\exp(\mathbf{q}_t^\top \mathbf{k}_j)}{\sum_{l=1}^t\exp(\mathbf{q}^\top_t \mathbf{k}_l)}\mathbf{v}_j &amp;&amp;\in \mathbb{R}^d \end{aligned}\] <p>Here,</p> <ul> <li> <p>\(L\) represents sequence length</p> </li> <li> <p>\(d\) represents head dimension</p> </li> <li> <p>\(\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O} \in \mathbb{R}^{L \times d}\) represent the query, key, value, and output matrices respectively.</p> </li> <li> <p>\(\mathbf{M} \in \mathbb{R}^{L \times L}\) is the causal mask for autoregressive modeling by ensuring each position can only attend to previous positions.</p> </li> </ul> <p>What linear attention<d-cite key="katharopoulos2020transformers"></d-cite> does is to simply remove the softmax operator <d-footnote>The original linear attention formulation incorporates feature mapping on queries and keys along with a normalizer term, but recent studies suggest these components may not be essential.<d-cite key="mao_fine-tuning_2022"></d-cite><d-cite key="sun2023retentive"></d-cite>.</d-footnote>:</p> \[\begin{aligned} \mathrm{Parallel\ training：} &amp;&amp;&amp;\mathbf{O}= (\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M})\mathbf{V} &amp;&amp;\in \mathbb{R}^{L\times d} \\ \mathrm{Iterative\ inference：}&amp;&amp;&amp;\mathbf{o_t} = \sum_{j=1}^t (\mathbf{q}_t^\top \mathbf{k}_j) \mathbf{v}_j &amp;&amp;\in \mathbb{R}^d \end{aligned}\] <p>While removing softmax alone doesn’t immediately reduce computational complexity, it enables a crucial mathematical property: linearity. This property, particularly associativity, allows us to restructure the computations in ways that significantly improve efficiency. For training, researchers have developed <strong>chunkwise parallel</strong> techniques<d-cite key="GAU"></d-cite><d-cite key="sun2023retentive"></d-cite><d-cite key="yang_gated_2023"></d-cite> that leverage this linearity to achieve subquadratic complexity while maintaining hardware efficiency, which forms the foundation of our open-source <strong>flash-linear-attention</strong> library<d-cite key="yang_fla_2024"></d-cite>.</p> <p>For inference, we can also rearrange the computation as follows:</p> <p>\(\begin{aligned} &amp;&amp;&amp;&amp;\mathbf{o_t} = \sum_{j=1}^t \mathbf{v}_j(\mathbf{k}_j^\top \mathbf{q}_t) &amp;&amp;&amp;&amp;&amp; \mathbf{k}_j^\top \mathbf{q}_t = \mathbf{q}_t^\top \mathbf{k}_j \in \mathbb{R}\\ &amp;&amp;&amp;&amp;= (\sum_{j=1}^t\mathbf{v}_j\mathbf{k}_j^\top)\mathbf{q}_t &amp;&amp;&amp;&amp;&amp;\text{By associativity} \end{aligned}\) $$</p> <p>Let’s define a state matrix \(\mathbf{S}_t = \sum_{j=1}^t\mathbf{v}_j\mathbf{k}_j^\top\). Then the computation can be expressed as:</p> \[\mathbf{S}_t = \mathbf{S}_{t-1} + \mathbf{v}_t\mathbf{k}_t^\top \in \mathbb{R}^{d\times d}, \quad \mathbf{o}_t = \mathbf{S}_t \mathbf{q}_t \in \mathbb{R}^{d}\] <p>This formulation reveals that linear attention is essentially a <strong>linear RNN with a matrix-valued state</strong> \(\mathbf{S}\) that accumulates key-value outer products, enabling efficient state (size) expansion from \(\mathcal{O}(d)\) to \(\mathcal{O}(d^2)\).</p> <details> <summary> Why do we want state expansion?</summary> Traditionally, RNN's hidden dimension is often the same (or of the same magnitude) as the input dimension, due to the expensive matrix-multiply-based state update. However, RNN solely relies on the recurrent state to remember the entire history and state size tends to be the bottleneck to remember sufficient amount of information, especially in retrieval tasks. We've been observing a substantial amount of research investigating hardware-efficient state expansion since Mamba1<d-cite key="Gu2023MambaLS"></d-cite> explicitly pointed it out, and linear attention styled outer-product-based update has proven to be optimal in terms of efficiently scaling state up (Mamba2<d-cite key="mamba2"></d-cite> also adopts this strategy!). In our previous HGRN2 work<d-cite key="qin_hgrn2_2024"></d-cite>, we investigated different approaches for state expansion, and the outer product based mechanism has proven to be both performant and scalable. </details> <p>With this approach, we only need to store and update \(\mathbf{S}_t\) instead of maintaining all previous key-value pairs. This optimization dramatically improves efficiency: the time complexity for autoregressive inference reduces from \(\mathcal{O}(L^2d)\) to \(\mathcal{O}(Ld^2)\), while the space complexity improves from \(\mathcal{O}(Ld)\) to \(\mathcal{O}(d^2)\). These improvements make this method particularly advantageous in two scenarios:</p> <ul> <li> <p><strong>Long sequence modeling</strong> where quadratic complexity of softmax attention could be a significant bottleneck.</p> </li> <li> <p>During <strong>generation</strong>, where computation is usually <strong>memory-bound</strong>, removing the KV cache can significantly enhance <strong>inference latency</strong> for \(L \gg d\).</p> </li> </ul> <h3 id="no-free-lunch-key-limitations-of-linear-attention">No Free Lunch: Key Limitations of Linear Attention</h3> <p>Unfortunately, there is no free lunch. The fixed-size state matrix in linear attention means it cannot perfectly preserve all historical information, making exact retrieval particularly challenging.</p> <p>More formally, linear attention implements a key-value associative memory, which is the sum of outer products between keys and values \(\mathbf{S} = \sum \mathbf{v}_i\mathbf{k}_i^\top\). Assuming all keys are normalized to unit length, when we try to retrieve a value associated with a specific key \(k_j\), we get:</p> \[\begin{aligned} \mathbf{S}\mathbf{k}_j &amp;= \sum \mathbf{v}_i (\mathbf{k}_i^\top \mathbf{k}_j) \\ &amp;= \mathbf{v}_j + \underbrace{\sum_{i\neq j} (\mathbf{k}_i^\top \mathbf{k}_j)\mathbf{v}_i}_{\text{retrieval error}} \end{aligned}\] <p>To minimize the retrieval error term, we need \(\mathbf{k}_i^\top \mathbf{k}_j = 0\) for all \(i\neq j\) - in other words, all keys should be <strong>orthogonal</strong> to each other. However, this reveals a fundamental limitation: in a \(d\)-dimensional space, you can only have at most \(d\) orthogonal vectors. This explains why increasing head dimension helps (For example, Sun et al.<d-cite key="sun2023retentive"></d-cite> have demonstrated the necessity of increasing head dimensions to enhance model performance) - it provides more “room” in the vector space for storing distinct key-value pairs!</p> <p>This theoretical limitation manifests in practice: vanilla linear attention has underperformed compared to softmax attention (by a large margin) in language modeling. The primary cause is memory “overload”: in this key-value associative memory system, we can only add new key-value associations without the ability to erase existing information. As sequences grow longer, this leads to accumulating “retrieval errors” that degrade performance. Indeed, as noted by David Eagleman in his book “Livewired: The Inside Story of the Ever-Changing Brain”,</p> <blockquote> <p>“The enemy of memory is not time; it’s other memories.”</p> </blockquote> <p>(Thanks to Kazuki Irie for the reference!). Recent advances in gated variants of linear attention (such as GLA<d-cite key="yang_gated_2023"></d-cite> and Mamba<d-cite key="Gu2023MambaLS"></d-cite>) have significantly narrowed the performance gap with standard attention in language modeling tasks by incorporating a <strong>forgetting mechanism</strong>. However, these models still face fundamental challenges with in-context retrieval and exact copying capabilities—limitations that have been both empirically observed and theoretically proven in recent work<d-cite key="zoology"></d-cite><d-cite key="arora_simple_2024"></d-cite><d-cite key="jelassi_repeat_2024"></d-cite>.</p> <details> <summary>Click here to learn more about gated variants of linear attention</summary> Given the close relationship between linear attention and RNN, it is no wonder that researchers want to enhance linear attention with the (forgetting) gating mechanisms, which has been shown unreasonably effective in nonlinear RNN<d-cite key="unreasonable-forget-gate"></d-cite> and linear RNN<d-cite key="HGRN"></d-cite>: <p> </p> <p> \[\mathbf{S}_t = \mathbf{G}_t \odot \mathbf{S}_{t-1} + \mathbf{v}_t\mathbf{k}_t^\top\] </p> <p> with different structured parameterization for \(\mathbf{G}_t \in \mathbb{R}^{d\times d}\) for parameter efficiency, often with outer product structure. Different models have proposed various ways to structure this gating matrix: </p> <p> For Decaying Fast weight<d-cite key="mao_fine-tuning_2022"></d-cite>: \[\mathbf{G}_t = \mathbf{\beta_t} \mathbf{\alpha_t}^\top\] </p> <p> For GLA<d-cite key="yang_gated_2023"></d-cite>: \[\mathbf{G}_t = \mathbf{1} \mathbf{\alpha_t}^\top\] </p> <p> For Mamba1<d-cite key="Gu2023MambaLS"></d-cite>: \[\mathbf{G}_t = \exp(-(\mathbf{\Delta_t} \mathbf{1}^\top) \odot \exp(A))\] </p> <p> For Mamba2<d-cite key="mamba2"></d-cite>: \[\mathbf{G}_t = \gamma_t \mathbf{1}\mathbf{1}^\top\] </p> <p> Cf. Table 1 of GLA<d-cite key="yang_gated_2023"></d-cite> for a summarization. </p> </details> <h2 id="deltanet-linear-attention-with-delta-rule">DeltaNet: Linear Attention with Delta Rule</h2> <h3 id="what-is-delta-rule">What is Delta Rule?</h3> <p>The Delta Rule<d-cite key="widrow_adaptive_1988"></d-cite> is a fundamental error-correction learning principle in neural networks. Its core idea is beautifully simple: adjust the model’s parameters based on the difference (delta) between what we want (target) and what we actually get (prediction).</p> <p>To understand this intuitively, imagine teaching a child to aim at a target. If they shoot too far to the left, you’d tell them to adjust right; too far right, adjust left. The size of the adjustment depends on how far they missed - a concept directly reflected in the Delta Rule.</p> <details> <summary>Click to expand Delta Rule code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">delta_rule</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Simple delta rule implementation
    x: input features (N samples by D features)
    y: target values (N samples)
    </span><span class="sh">"""</span>
    <span class="c1"># Initialize weights
</span>    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Train
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
            <span class="c1"># Forward pass
</span>            <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span>
            
            <span class="c1"># Compute error
</span>            <span class="n">error</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred</span>
            
            <span class="c1"># Update weights
</span>            <span class="n">w</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
    <span class="k">return</span> <span class="n">w</span>

<span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Generate toy data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 100 samples, 3 features
</span>    <span class="n">true_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_w</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    
    <span class="c1"># Train
</span>    <span class="n">w</span> <span class="o">=</span> <span class="nf">delta_rule</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">True weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">true_w</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Learned weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></code></pre></figure> </details> <h3 id="what-is-deltanet">What is DeltaNet?</h3> <p>DeltaNet<d-cite key="schlag_linear_2021"></d-cite> applies this error-correction principle to linear attention. Instead of simply accumulating key-value outer product, it updates its state based on prediction errors:</p> \[\begin{align*} \mathbf{S}_{t} &amp;= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \end{align*}\] <p>The parallel to the Delta Rule becomes clear when we break down the components:</p> <ul> <li>\(\beta_t \in \mathbb{R}\) acts as the learning rate</li> <li>\(\mathbf{k}_t \in \mathbb{R}^d\) is the input data</li> <li>\(\mathbf{v}_t \in \mathbb{R}^d\) is the target</li> <li>\(\mathbf{S}_{t-1} \mathbf{k}_t \in \mathbb{R}^d\) is our current prediction</li> </ul> <p>We will revisit this form later, showing how it can emerge naturally from a single gradient descent step on a (online) loss function.</p> <p>There’s another intuitive way to understand this update rule. Think of \(\mathbf{S}_{t-1}\mathbf{k}_t\) as retrieving the “old value” associated with the current key \(\mathbf{k}_t\) from memory. When we encounter a newly associated value \(\mathbf{v}_t\) for the same key, rather than blindly overwriting, we make a careful update:</p> \[\begin{align*} \mathbf{v}_t^{\text{new}} &amp;= (1-\beta_t) \mathbf{v}_t^{\text{old}} + \beta_t \mathbf{v}_t, \\ \mathbf{S}_t &amp;= \mathbf{S}_{t-1} - \underbrace{\mathbf{v}_t^{\text{old}} \mathbf{k}_t^\top}_{\text{erase}} + \underbrace{\mathbf{v}_t^{\text{new}} \mathbf{k}_t^\top}_{\text{write}} \end{align*}\] <p>where \(\mathbf{v}_t^{\text{new}}\) is a learned combination of the old and current values, controlled by a dynamic \(\beta_t \in (0,1)\): when \(\beta_t=0\), the memory content remains intact, and when \(\beta_t=1\), we completely replace the old associated value with the new one.</p> <h3 id="deltanet-as-a-strong-in-context-learning-rnn">DeltaNet as a Strong In-context Learning RNN</h3> <p>MQAR (Multi-Query Associative Recall)<d-cite key="zoology"></d-cite> is a recent popular synthetic benchmark aimed at measuring the in-context associative recall ability for subquadratic models.</p> <p>The MQAR task works as follows: Each letter is associated with a number, and the model is asked to correctly recall the number associated with each letter in a query sequence.</p> <p>For example, given the input:</p> <p><code class="language-plaintext highlighter-rouge">A 4 B 3 C 6 F 1 E 2 → A ? C ? F ? E ? B ?</code></p> <p>The format consists of:</p> <ol> <li>Key-Value pairs (before the arrow): Letters paired with their corresponding numbers</li> <li>Query sequence (after the arrow): Letters whose associated numbers need to be recalled</li> </ol> <p>The correct output for this example would be:</p> <p><code class="language-plaintext highlighter-rouge">4, 6, 1, 2, 3</code></p> <p>While conventional gated convolution and recurrent models generally underperform in this task, in our experiments, we show that DeltaNet <d-footnote>Interestingly, DeltaNet was initially designed to improve associative recall performance but remained largely overlooked until this work.</d-footnote> demonstrates notably strong performance:</p> <div class="row justify-content-center"> <div class="col-6"> <img class="img-fluid" style="background-color: white; padding: 20px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" src="/assets/img/blog/deltanet/mqar-1.png"/> </div> </div> <div class="caption"> The hardest setting from the original Zoology paper </div> <p>This initial success was particularly exciting—achieving perfect performance on MQAR exceeded our expectations. What makes this result especially promising is that MQAR performance strongly correlates with “Associative-Recall-Hit” in real-world language modeling tasks<d-cite key="zoology"></d-cite>. Associative recall failures are a primary source of errors in subquadratic models and largely account for their perplexity gap relative to softmax attention. Thus, DeltaNet’s perfect MQAR performance suggested significant potential.</p> <p>We’ve also conducted experiments on MAD<d-cite key="poli_mechanistic_2024"></d-cite>, another more comprehensive benchmark than MQAR that is also motivated to test new architecture’s capacities, and the results are summarized below:</p> <table> <thead> <tr> <th>Model</th> <th>Compress</th> <th>Fuzzy Recall</th> <th>In-Context Recall</th> <th>Memorize</th> <th>Noisy Recall</th> <th>Selective Copy</th> <th>Average</th> </tr> </thead> <tbody> <tr> <td>Transformer</td> <td>51.6</td> <td>29.8</td> <td>94.1</td> <td>85.2</td> <td>86.8</td> <td>99.6</td> <td>74.5</td> </tr> <tr> <td>Hyena</td> <td>45.2</td> <td>7.9</td> <td>81.7</td> <td>89.5</td> <td>78.8</td> <td>93.1</td> <td>66.0</td> </tr> <tr> <td>Multihead Hyena</td> <td>44.8</td> <td>14.4</td> <td>99.0</td> <td>89.4</td> <td>98.6</td> <td>93.0</td> <td>73.2</td> </tr> <tr> <td>Mamba</td> <td>52.7</td> <td>6.7</td> <td>90.4</td> <td>89.5</td> <td>90.1</td> <td>86.3</td> <td>69.3</td> </tr> <tr> <td>GLA</td> <td>38.8</td> <td>6.9</td> <td>80.8</td> <td>63.3</td> <td>81.6</td> <td>88.6</td> <td>60.0</td> </tr> <tr> <td>DeltaNet</td> <td>42.2</td> <td>35.7</td> <td>100</td> <td>52.8</td> <td>100</td> <td>100</td> <td>71.8</td> </tr> </tbody> </table> <p>where DeltaNet demonstrates its strong in-context recall capacities. These synthetic tasks are inexpensive to run and offer clear evidence that DeltaNet is likely to perform well at scale. This motivated us to focus on developing DeltaNet’s training algorithm and kernel implementation—after all, scaling up an arbitrary architecture without demonstrating its potential would risk wasting significant time and resources.</p> <p>In the next post, we’ll explore a beautiful algorithm that parallelizes DeltaNet across sequence length. But first, let’s build some intuition about why DeltaNet is particularly well-suited for in-context retrieval tasks.</p> <h3 id="why-is-deltanet-superior-at-in-context-retrieval-compared-to-linear-attention">Why is DeltaNet Superior at In-context Retrieval Compared to Linear Attention?</h3> <p>DeltaNet’s update rule can be derived by sequentially minimizing the mean squared error (MSE) between the desired output and the predicted output at each time step \(t\) using gradient descent: <d-footnote>This formulation reveals an interesting connection to Test-Time-Training (TTT) <d-cite key="sun-2024-learning"></d-cite>: DeltaNet becomes mathematically equivalent to TTT-linear under two specific conditions: (1) when nonlinear components such as layer normalization are removed, and (2) when the mini-batch size in TTT is set to one.</d-footnote></p> \[\mathcal{L}_t(\mathbf{S}) = \frac{1}{2}\|\mathbf{S} \mathbf{k}_t - \mathbf{v}_t\|^2\] <p>Applying gradient descent to minimize this MSE loss gives:</p> \[\begin{aligned} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} - \eta_t \nabla \mathcal{L}_t(\mathbf{S}_{t-1}) \\ &amp;= \mathbf{S}_{t-1} - \eta_t \left(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t\right) \mathbf{k}_t^\top \end{aligned}\] <p>When the learning rate \(\eta_t\) is set to \(\beta_t\), this results in DeltaNet’s update rule.</p> <p>In contrast, vanilla linear attention employs a linear loss function:</p> \[\mathcal{L}^\prime_t(\mathbf{S}) = -\langle \mathbf{S} \mathbf{k}_t, \mathbf{v}_t \rangle\] <p>The corresponding update rule for linear attention is:</p> \[\begin{aligned} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} - \eta_t \nabla \mathcal{L}_t^\prime(\mathbf{S}_{t-1}) \\ &amp;= \mathbf{S}_{t-1} + \eta_t \mathbf{v}_t \mathbf{k}_t^\top \end{aligned}\] <p>By setting \(\eta_t = 1\), the standard linear attention update is recovered.</p> <p>Thus, DeltaNet’s superior performance in in-context retrieval becomes evident—it minimizes MSE at each step, making it ideal for tasks like associative recall where reducing large errors is crucial for accurate retrieval.</p>]]></content><author><name>Songlin Yang</name></author><summary type="html"><![CDATA[A gentle and comprehensive introduction to the DeltaNet]]></summary></entry><entry><title type="html">DeltaNet Explained (Part II)</title><link href="https://sustcsonglin.github.io/blog/2024/deltanet-2/" rel="alternate" type="text/html" title="DeltaNet Explained (Part II)"/><published>2024-12-03T22:25:00+00:00</published><updated>2024-12-03T22:25:00+00:00</updated><id>https://sustcsonglin.github.io/blog/2024/deltanet-2</id><content type="html" xml:base="https://sustcsonglin.github.io/blog/2024/deltanet-2/"><![CDATA[<p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></strong> (w/ <a href="https://berlino.github.io/">Bailin Wang</a>, <a href="https://yzhang.site/">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/">Yoon Kim</a>). <strong>You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="#">Part II - The Algorithm</a></li> <li><a href="/blog/2024/deltanet-3/">Part III - The Neural Architecture</a></li> </ol> <h2 id="parallel-scan-for-deltanet-a-failed-attempt">Parallel Scan for DeltaNet: A Failed Attempt</h2> <p>Ok, we’ve seen in the previous section that DeltaNet does really well on these diagnostic synthetic tasks. So now we just need to scale it up to modern LMs, right? Well, it turns out it’s not that simple. In particular, the original DeltaNet treated DeltaNet as a pure RNN which required O(L) sequential steps, which is inefficient on modern hardware such as GPUs with massive parallel processing capabilities. We thus seek strategies to parallelize DeltaNet across sequence length to enable hardware-efficient training. In this post, we first discuss parallel scan as a interesting-but-impractical strategy for parallelizing DeltaNet. We then give another algorithm for parallelization that is more efficient in practice.</p> <h3 id="from-delta-updates-to-matrix-multiplication-form">From Delta Updates to Matrix Multiplication Form</h3> <p>Let’s start with DeltaNet’s original state update equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top\] <p>To transform this into a matrix multiplication form, let’s expand the equation step by step:</p> \[\begin{align*} \mathbf{S}_{t} &amp;= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \end{align*}\] <p>For simplicity, let’s denote:</p> <ul> <li>\(\mathbf{M}_t = \mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal\) as our transition matrix</li> <li>\(\mathbf{X}_t = \beta_t \mathbf{v}_t \mathbf{k}_t^\top\) as our update term</li> </ul> <p>Then our update becomes:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1}\mathbf{M}_t + \mathbf{X}_t \in \mathbb{R}^{d\times d}\] <h3 id="defining-the-associative-operator">Defining the Associative Operator</h3> <p>This form matches exactly with the first-order recurrence shown in equation (1.5) from <em>Prefix Sums and Their Applications</em><d-cite key="Blelloch1990PrefixSA"></d-cite>, where matrix multiplication (⊗) and matrix addition (⊕) serve as our binary operators. Both operators satisfy the required properties:</p> <ol> <li>Matrix addition is associative: \((A + B) + C = A + (B + C)\)</li> <li>Matrix multiplication is associative: \((AB)C = A(BC)\)</li> <li>Matrix multiplication distributes over addition: \(A(B + C) = AB + AC\)</li> </ol> <p>Following the framework, we define our state pairs as:</p> \[c_t = [\mathbf{M}_t, \mathbf{X}_t] = [\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal, \beta_t \mathbf{v}_t \mathbf{k}_t^\top]\] <p>And our associative operator • that combines these pairs:</p> \[c_i \bullet c_j = [\mathbf{M}_i\mathbf{M}_j, \mathbf{M}_j\mathbf{X}_i + \mathbf{X}_j]\] <p>This operator definition preserves the temporal dependencies of our updates - when we combine two steps, the earlier update term \(\mathbf{X}_i\) must be transformed by the later transition matrix \(\mathbf{M}_j\), while the later update term \(\mathbf{X}_j\) remains unchanged.</p> <h3 id="parallel-scan-for-deltanet">Parallel Scan for DeltaNet</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/scan.png" alt="示例图片" style="width: 99%"/> </div> </div> <p>With this associative operator, we can use parallel scan to compute all states in parallel. The algorithm works in two phases:</p> <h5 id="sweep-down-phase">Sweep-Down Phase</h5> <p>First, we compute partial results in parallel by combining adjacent pairs:</p> <p>For steps 0 and 1, we compute:</p> \[c_1 = c_0 \bullet c_1 = [\mathbf{M}_0\mathbf{M}_1, \mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1]\] <p>Similarly for steps 2 and 3:</p> \[c_3 = c_2 \bullet c_3 = [\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <p>Then combine these results:</p> \[c_{1:3} = c_{1} \bullet c_{3} = [\mathbf{M}_0\mathbf{M}_1\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_2\mathbf{M}_3(\mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1) + \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <h5 id="sweep-up-phase">Sweep-Up Phase</h5> <p>In this phase, we use our partial results to compute intermediate states:</p> \[c_2 = c_1 \bullet c_2 = [\mathbf{M}_1\mathbf{M}_2, \mathbf{M}_2\mathbf{X}_1 + \mathbf{X}_2]\] <p>This parallelization transforms DeltaNet’s sequential state updates into an efficient parallel computation, reducing the sequential dependency chain from \(\mathbf{O}(L)\) to \(\mathcal{O}(\log L)\) steps while maintaining mathematical equivalence.</p> <h3 id="whats-wrong-with-parallel-scan-for-deltanet">What’s Wrong with Parallel Scan for DeltaNet?</h3> <p>Despite parallelizability, parallel scan for DeltaNet faces two major challenges: computational complexity and memory requirements.</p> <p>The first issue lies in the <strong>time complexity</strong>. For DeltaNet, parallel scan yields \(\mathcal{O}(L\log L d^3)\) complexity due to the cubic cost of matrix multiplication when treating \(\mathbf{M}_t\) as dense matrices. At first glance, we might think we can leverage the identity-plus-low-rank structure of \(\mathbf{M}_t\) for acceleration. Let’s work through this carefully.</p> <p>When multiplying two adjacent matrices, we get:</p> \[\begin{align*} (\mathbf{I}-\beta_0 \mathbf{k}_0 \mathbf{k}_0^\top)(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) &amp;= \mathbf{I}(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) \\ &amp;= (\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 \mathbf{k}_0^\top \mathbf{k}_1 \mathbf{k}_1^\top \\ &amp;= \mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top \end{align*}\] <p>This computation reduces the complexity from \(\mathcal{O}(d^3)\) to \(\mathcal{O}(d^2)\) by leveraging the identity-plus-low-rank structure - we only need to compute vector inner products \((\mathbf{k}_0^\top \mathbf{k}_1)\) and outer products between vectors. Similarly for the next pair:</p> \[\begin{align*} (\mathbf{I}-\beta_2 \mathbf{k}_2 \mathbf{k}_2^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top) &amp;= \mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top \end{align*}\] <p>When we try to combine these results to compute larger spans like \(c_{1:4}\), the multiplication becomes increasingly complex. We need to multiply:</p> \[(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top)\] <p>When we try to combine these results to compute larger spans like \(c_{1:4}\), the multiplication becomes increasingly complex. We need to multiply:</p> \[(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top)\] <p>Each term in the first bracket must multiply with each term in the second bracket. While each matrix is initially a sum of \(O(1)\) rank-1 terms, this multiplication leads to a quadratic growth in the number of terms. After \(\log L\) levels of parallel scan, we end up with \(O(L^{\log c})\) terms, where \(c\) is the initial number of terms per matrix. This exponential growth in the number of terms, despite each being rank-1, makes maintaining the explicit structure impractical. Therefore, treating these as dense matrices with \(O(d^3L \log L)\) complexity becomes a more reasonable approach, especially considering the efficiency of dense matrix operations on modern hardware. This explains why parallel scan, while theoretically appealing, faces significant practical challenges for DeltaNet computation.</p> <p>The second major issue is <strong>space complexity</strong>. Parallel scan requires materializing all intermediate d×d matrices at each step to high-bandwidth memory (HBM). For linear RNNs with matrix-valued states, this materialization becomes prohibitively expensive (\(\mathcal{O}(Ld^2)\)). While recurrent computation can avoid such materialization <d-footnote> See section 3.3.1 of Katharopoulos et al.<d-cite key="katharopoulos2020transformers"></d-cite> for more details.</d-footnote>, parallel scan offers no apparent workaround unless all states fit into SRAM, as implemented in Mamba’s hardware-aware selective scan algorithm that eliminates the need for materialization. However, this approach imposes limitations on state size - too large a state leads to out-of-shared-memory issues. Given that I/O costs dominate this computation, parallel scan may become undesirable in practice. As noted in recent discussions:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dear parallel-scan people.<br/><br/>Q(t) and K(t) are 1xD and V(t) is 1xC, with D and T large, and I want to compute<br/><br/>Y(t) = Q(t)M(t)<br/><br/>with<br/><br/>M(0)=0<br/>M(t+1) = M(t) + K(t)^T V(t)<br/><br/>A naive non-parallel scan approach is O(T) in time but I do not have to store any DxD<br/><br/>1/2</p>&mdash; François Fleuret (@francoisfleuret) <a href="https://twitter.com/francoisfleuret/status/1793016689589625263?ref_src=twsrc%5Etfw">May 21, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">The key idea is to only materialize chunk-level hidden states, using matmuls to calculate outputs based on the query, key, and value matrices and chunk-level hidden states. This method avoids materializing the hidden state for every single timestep.</p>&mdash; Songlin Yang (@SonglinYang4) <a href="https://twitter.com/SonglinYang4/status/1793029555277697379?ref_src=twsrc%5Etfw">May 21, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Here I previously discussed the chunkwise algorithm - another type of associative scan that offers improved memory efficiency and better utilization of tensor cores by enabling more matrix multiplication operations (for a detailed analysis, see <d-cite key="yang_gated_2023"></d-cite>). Given these advantages, developing a chunkwise training algorithm for DeltaNet that maintains quadratic complexity with respect to \(d\) while preserving memory efficiency would be highly valuable.</p> <h2 id="a-chunkwise-algorithm-for-deltanet">A Chunkwise Algorithm for DeltaNet</h2> <h3 id="chunkwise-parallel-form-for-linear-attention">Chunkwise Parallel Form for Linear Attention</h3> <p>Linear attention’s efficiency stems from its ability to maintain a compact representation of the state using vectors rather than materializing full matrices. This is possible because a sum of outer products can be rewritten as matrix multiplication:</p> \[\begin{align*} \sum_{i=1}^t \mathbf{v}_i \mathbf{k}_i^\top &amp;= \mathbf{V}_t\mathbf{K}_t^\top \\ \text{where } \mathbf{V}_t &amp;= [\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_t] \\ \mathbf{K}_t &amp;= [\mathbf{k}_1, \mathbf{k}_2, ..., \mathbf{k}_t] \end{align*}\] <p>This matrix multiplication form is highly optimized on modern GPUs with tensor cores. Leveraging this property, instead of storing all intermediate hidden states, we can store states only at regular intervals of size \(C\) as checkpoints. This gives us states \(\mathbf{S}_{0}, \mathbf{S}_{C}, \mathbf{S}_{2C}, ..., \mathbf{S}_{(n-1)C}\) where \(n = \lceil L/C \rceil\).</p> <p>Denoting \(\mathbf{S}_{[i]} := \mathbf{S}_{iC} \in \mathbb{R}^{d \times d}\); \(\square_{[i]} = \square_{iC+1:(i+1)C} \in \mathbb{R}^{C \times d}\) for \(\square \in \{\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}\}\); \(\square_{[i]}^r = \square_{iC+r}\) for \(\square \in \{\mathbf{q}, \mathbf{k}, \mathbf{v}, \mathbf{o}, \mathbf{S}\}\). For any position r within chunk i, we can compute:</p> \[\begin{align*} \mathbf{S}_{[i]}^r &amp;= \mathbf{S}_{[i]} + \sum_{t=1}^{r} \mathbf{v}_{[i]}^t \mathbf{k}_{[i]}^{t\top} \\ \mathbf{o}_{[i]}^r &amp;= \mathbf{S}_{[i]}^r \mathbf{q}_{[i]}^r = \mathbf{S}_{[i]}\mathbf{q}_{[i]}^r + \sum_{t=1}^{r} \mathbf{v}_{[i]}^t (\mathbf{k}^{t\top}_{[i]} \mathbf{q}_{[i]}^r) \end{align*}\] <p>and in matrix form,</p> \[\begin{align*} \mathbf{S}_{[t+1]} &amp;= \mathbf{S}_{[t]} + \mathbf{V}_{[t]}^\top \mathbf{K}_{[t]} &amp;&amp; \in \mathbb{R}^{d\times d} \\ \mathbf{O}_{[t]} &amp;= \mathbf{Q}_{[t]} \mathbf{S}_{[t]}^\top + (\mathbf{Q}_{[t]}\mathbf{K}_{[t]}^\top \odot \mathbf{M}) \mathbf{V}_{[t]} &amp;&amp; \in \mathbb{R}^{C\times d} \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/chunk-linear-attn.png" alt="示例图片" style="width: 99%"/> <figcaption style="margin-top: 10px; color: #666; text-align: center;"> A visual representation of the chunkwise algorithm for linear attention </figcaption> </div> </div> <p>This chunkwise formulation enables efficient hardware utilization by leveraging tensor cores when the chunk size C is a multiple of 16, as implemented in our open-source library <strong>flash-linear-attention</strong><d-cite key="yang_fla_2024"></d-cite>.</p> <h3 id="wy-representation-for-deltanet">WY representation for DeltaNet</h3> <p>However, as we saw in our failed attempt above, the cumulative product of DeltaNet’s transition matrices seemed to resist such compact representation, apparently requiring us to store numerous intermediate results. Fortunately, there’s a solution: DeltaNet’s transition matrices closely resemble <a href="https://en.wikipedia.org/wiki/Householder_transformation">Householder matrices</a> (when \(\beta_t\)=2), and there exists an elegant compact representation for their cumulative product.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/householder.png" alt="示例图片" style="width: 49%"/> <figcaption style="margin-top: 10px; color: #666;"> A visual representation of the Householder reflector transformation </figcaption> </div> </div> <p>This representation, known as the WY representation, was introduced in a seminal 1985 paper<d-cite key="bischof_wy_1985"></d-cite>. Using DeltaNet’s notation, the cumulative product can be written as:</p> \[\prod_{i=1}^{t} (\mathbf{I} - \beta_i \mathbf{k}_i \mathbf{k}_i^\top) = \mathbf{I} - \sum_{i=1}^t \mathbf{w}_i\mathbf{k}_i^\top\] <p>We can prove this by mathematical induction. Let’s define \(\mathbf{P}_n = \prod_{t=1}^n(\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\top)\). For n=1, the equation clearly holds. Assuming it holds for n-1, we can prove it for n:</p> \[\begin{align*} \mathbf{P}_n &amp;= \mathbf{P}_{n-1} (\mathbf{I} - \beta_n \mathbf{k}_n \mathbf{k}_n^\top) \\ &amp;= (\mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top)(\mathbf{I} - \beta_n \mathbf{k}_n \mathbf{k}_n^\top) \\ &amp;= \mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top - \beta_n \mathbf{k}_n \mathbf{k}_n^\top + (\sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top) \beta_n \mathbf{k}_n \mathbf{k}_n^\top \\ &amp;= \mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top - \underbrace{\left(\beta_n \mathbf{k}_n - \beta_n \sum_{t=1}^{n-1} \left(\mathbf{w}_t (\mathbf{k}_t^\top\mathbf{k}_n)\right) \right)}_{\mathbf{w}_n}\mathbf{k}_n^\top \\ &amp;= \mathbf{I} - \sum_{t=1}^n \mathbf{w}_t\mathbf{k}_t^\top \end{align*}\] <p>This proof not only establishes the correctness of the representation but also provides a constructive way to compute the \(\mathbf{w}\) vectors!</p> <p>Similarly, we can show \(\mathbf{S}_n = \sum_{t=1}^{n} \mathbf{u}_t \mathbf{k}_n^\top\) by induction:</p> \[\begin{align*} \mathbf{S}_n &amp;= \mathbf{S}_{n-1} (\mathbf{I} - \beta_n \mathbf{k}_n\mathbf{k}_n^\top) + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \left(\sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top\right) (\mathbf{I} - \beta_n \mathbf{k}_n\mathbf{k}_n^\top) + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top - \left(\sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top\right) \beta_n \mathbf{k}_n \mathbf{k}_n^\top + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top + \underbrace{\left(\beta_n \mathbf{v}_n - \beta_n\sum_{t=1}^{n-1} \mathbf{u}_t \left(\mathbf{k}_t^\top \mathbf{k}_n \right) \right)}_{\mathbf{u}_n} \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n} \mathbf{u}_t \mathbf{k}_n^\top \end{align*}\] <p>Looking at this sum-of-outer-products structure, we can see it closely resembles linear attention’s update form. This similarity suggests a path toward developing a novel parallel algorithm!</p> <h3 id="chunkwise-parallel-form-for-deltanet">Chunkwise Parallel Form for DeltaNet</h3> <p>First, let’s unroll the recurrence of DeltaNet:</p> \[\begin{align*} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\top) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \sum_{i=1}^t \beta_i (\mathbf{v}_i \mathbf{k}_i^\top) \left(\prod_{j=i+1}^t (\mathbf{I} - \beta_j \mathbf{k}_j \mathbf{k}_j^\top)\right) \end{align*}\] <p>Similar to linear attention, we can use checkpointing to store states at regular intervals of size C. For any position r within chunk i, we have:</p> \[\begin{align*} \mathbf{S}_{[i]}^r &amp;= \mathbf{S}_{[i]} \underbrace{\prod_{t=1}^{r} (\mathbf{I}-\beta_{[i]}^t\mathbf{k}_{[i]}^t\mathbf{k}_{[i]}^{t\top})}_{\text{chunk-local cumprod: } \mathbf{P}_{[i]}^r} + \underbrace{\sum_{t=1}^{r} (\beta_{[i]}^t \mathbf{v}_{[i]}^t \mathbf{k}_{[i]}^{t\top} \prod_{s=t+1}^{r} (\mathbf{I}-\beta_{[i]}^s\mathbf{k}_{[i]}^s\mathbf{k}_{[i]}^{s\top}))}_{\text{chunk-local state or cumprodsum: }\mathbf{H}_{[i]}^r} \\ &amp;= \mathbf{S}_{[i]} (\mathbf{I} - \sum_{t=1}^r\mathbf{w}_{[i]}^t\mathbf{k}_{[i]}^{t\top}) + \sum_{t=1}^r \mathbf{u}_{[i]}^t \mathbf{k}_{[i]}^{t\top}\\ \end{align*}\] <p>where \(\mathbf{w}_{[i]}^t\) and \(\mathbf{u}_{[i]}^t\) are computed using the WY representation, but starting from the first position of each chunk rather than the beginning of the sequence, enabling parallel computation across chunks.</p> \[\mathbf{w}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{k}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{w}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] \[\mathbf{u}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{v}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] <p>And for output computation,</p> \[\begin{align*} \mathbf{o}_{[i]}^r &amp;= \mathbf{S}_{[i]}^r \mathbf{q}_{[i]}^r \\ &amp;= \mathbf{S}_{[i]} \mathbf{q}_{[i]}^r - \sum_{t=1}^r \mathbf{S}_{[i]}\mathbf{w}_{[i]}^t(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) + \sum_{t=1}^r \mathbf{u}_{[i]}^t(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) \\ &amp;= \mathbf{S}_{[i]} \mathbf{q}_{[i]}^r + \sum_{t=1}^r (\mathbf{u}_{[i]}^t - \mathbf{S}_{[i]}\mathbf{w}_{[i]}^t)(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) \end{align*}\] <p>Together, in matrix-multiplication form,</p> \[\begin{align*} \mathbf{S}_{[i+1]} &amp;= \mathbf{S}_{[i]} (\mathbf{I}-\mathbf{W}_{[i]}^\top \mathbf{K}_{[i]}) + \mathbf{U}_{[i]}^\top \mathbf{K}_{[i]} \\ &amp;= \mathbf{S}_{[i]} + \left(\mathbf{U}_{[i]} - \mathbf{W}_{[i]}\mathbf{S}_{[i]}^\top\right)^\top \mathbf{K}_{[i]} &amp;&amp; \in \mathbb{R}^{d\times d} \\ \mathbf{O}_{[i]} &amp;= \mathbf{Q}_{[i]} \mathbf{S}_{[i]}^\top + (\mathbf{Q}_{[i]} \mathbf{K}_{[i]}^\top \odot \mathbf{M}) \left(\mathbf{U}_{[i]} - \mathbf{W}_{[i]} \mathbf{S}_{[i]}^\top\right) &amp;&amp; \in \mathbb{R}^{C\times d} \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-chunk.png" alt="示例图片" style="width: 99%"/> <figcaption style="margin-top: 10px; color: #666; text-align: center;"> A visual representation of the chunkwise algorithm for DeltaNet </figcaption> </div> </div> <h3 id="ut-transform-through-the-lens-of-graph-theory">UT Transform Through the Lens of Graph Theory</h3> <p>The chunkwise parallel form transforms most of DeltaNet’s operations into efficient matrix multiplications similar to linear attention. However, there’s a key computational bottleneck: the recursive construction of update vectors \(\mathbf{U}_{[i]}\) and \(\mathbf{W}_{[i]}\). This motivates the need for the UT transform<d-cite key="Joffrain2006AccumulatingHT"></d-cite> - to restructure this recursive computation into a form that can leverage efficient matrix multiplication. Let’s understand this through graph theory.</p> <p>In graph theory, for a weighted directed graph, the adjacency matrix \(\mathbf{A}\) captures direct connections - entry \(\mathbf{A}[i,j]\) represents the edge weight from node \(j\) to node \(i\). When we compute \((\mathbf{I} - \mathbf{A})^{-1}\), each entry \([i,j]\) gives the sum of weights of all possible paths from \(j\) to \(i\).</p> <p>Looking at our recursive update equations:</p> \[\mathbf{w}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{k}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] \[\mathbf{u}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{v}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] <p>These form a weighted directed graph where:</p> <ul> <li>Nodes represent sequence positions</li> <li>Directed edges connect position \(i\) to \(r\) where \(i &lt; r\) (causal dependency)</li> <li>Edge weights \(-\beta_{[t]}^r\mathbf{k}_{[t]}^{i\top}\mathbf{k}_{[t]}^r\) encode interactions through key similarity and learning rate</li> </ul> <p>This graph structure can be represented by the adjacency matrix, which could be computed efficiently:</p> \[\mathbf{A}_{[t]} = \operatorname{tril}(-\operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{K}_{[t]} \mathbf{K}_{[t]}^\top,-1)\] <p>Since \(\mathbf{A}_{[t]}\) is strictly lower triangular, \((\mathbf{I} - \mathbf{A}_{[t]})\) is also lower triangular with ones on the diagonal. This special structure allows us to efficiently compute its inverse through forward substitution:</p> \[\mathbf{T}_{[t]} = \left(\mathbf{I} - \mathbf{A}_{[t]}\right)^{-1}\] <p>This avoids the need for general matrix inversion, making the computation much more efficient. After obtaining \(\mathbf{T}_{[t]}\), which captures all accumulated influence paths between positions, we proceed with the final multiplication:</p> \[\mathbf{W}_{[t]} = \mathbf{T}_{[t]} \operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{K}_{[t]}, \quad \mathbf{U}_{[t]}=\mathbf{T}_{[t]}\operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{V}_{[t]}\] <p>Applies these accumulated influences to compute our updates in a hardware-efficient form.</p> <h3 id="speed-comparison">Speed comparison</h3> <p>We implemented both the recurrent and chunkwise parallel versions of DeltaNet using Triton. Our experiments compare their performance across different sequence lengths (\(L\)) and head dimensions (\(d_{\text{head}}\)), with a fixed model dimension \(d=2048\). To ensure fair comparison across configurations, we kept the total sequence elements constant at 16,384 by adjusting batch sizes accordingly.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/speedup.png" alt="示例图片" style="width: 60%"/> </div> </div> <p>As we can see in the figure above, our chunkwise parallel approach consistently outperforms the recurrent baseline. More importantly, this performance advantage grows more pronounced under two key conditions: as sequences get longer and as head dimensions increase. To understand why, let’s examine two fundamental limitations of recurrent implementations that our approach addresses.</p> <p>The first limitation concerns parallelism strategy. Recurrent implementations process sequences step-by-step, relying primarily on two sources of parallelism to keep GPU cores busy: the batch dimension (processing multiple sequences simultaneously) and the head dimension (computing multiple attention heads in parallel). While this strategy worked well with moderate sequence lengths and larger batch sizes, it faces challenges in modern training scenarios. Today’s models increasingly work with longer sequences or larger model parameters, often necessitating smaller batch sizes for memory efficiency. This shift was notably highlighted in the FlashAttention2 paper<d-cite key="flashattention2"></d-cite>, which identified sequence-level parallelism as crucial for training. Without the ability to parallelize across the sequence dimension, recurrent implementations hit a fundamental bottleneck: when the product of batch size and number of attention heads is small, they can’t provide enough parallel work to keep modern GPUs fully utilized. This results in low occupancy of Streaming Multiprocessors (SMs) and suboptimal speed performance.</p> <p>The second limitation relates to hardware utilization. Modern GPUs include specialized tensor cores designed to accelerate matrix multiplication operations, offering up to 16x speedup for half-precision computations compared to other operations with equivalent FLOP counts. Recurrent implementations, despite requiring fewer total FLOPs, struggle to effectively leverage these hardware accelerators. This becomes particularly problematic with larger head dimensions, which are often necessary for tasks requiring substantial memory capacity (like in-context retrieval). Our chunkwise implementation, in contrast, restructures the computation to maximize use of tensor cores, achieving better real-world performance despite higher theoretical FLOP counts. This performance analysis illustrates a crucial principle in modern hardware-efficient deep learning: raw FLOP counts don’t always translate directly to wall-clock time. The ability to leverage specialized hardware accelerators and maintain high GPU utilization often matters more than theoretical operation counts. Our chunkwise implementation succeeds by aligning the computation with these hardware realities.</p> <p>Finally, we compare DeltaNet’s training throughput against other models at the 1.3B parameter scale.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/throughputs.png" alt="示例图片" style="width: 75%"/> </div> </div> <p>DeltaNet achieves competitive throughput, running only slightly slower than GLA (Gated Linear Attention). This small performance gap is a reasonable trade-off for DeltaNet’s more expressive transition matrices.</p>]]></content><author><name>Songlin Yang</name></author><summary type="html"><![CDATA[An algorithm that parallelizes DeltaNet computation across the sequence length dimension]]></summary></entry><entry><title type="html">DeltaNet Explained (Part III)</title><link href="https://sustcsonglin.github.io/blog/2024/deltanet-3/" rel="alternate" type="text/html" title="DeltaNet Explained (Part III)"/><published>2024-12-03T22:25:00+00:00</published><updated>2024-12-03T22:25:00+00:00</updated><id>https://sustcsonglin.github.io/blog/2024/deltanet-3</id><content type="html" xml:base="https://sustcsonglin.github.io/blog/2024/deltanet-3/"><![CDATA[<p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></strong> (w/ <a href="https://berlino.github.io/">Bailin Wang</a>, <a href="https://yzhang.site/">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/">Yoon Kim</a>). <strong>You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="/blog/2024/deltanet-2/">Part II - The Algorithm</a></li> <li><a href="#">Part III - The Neural Architecture</a></li> </ol> <h2 id="deltanet-architecture-design">DeltaNet architecture design</h2> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-net-arch.png" alt="示例图片" style="width: 99%"/> <figcaption style="margin-top: 10px; color: #666;"> DeltaNet architecture </figcaption> </div> </div> <p>In this final post, we explore our modernization of DeltaNet’s architecture. While maintaining the core delta rule mechanism, we’ve introduced several architectural improvements that significantly enhance its performance.</p> <p>At a high level, DeltaNet follows the modern transformer block design popularized by Llama, alternating between token mixing (DeltaNet replacing self-attention) and channel mixing (SwiGLU). Our main architectural modifications focus on the token mixing layer, where we introduce three key improvements. First, we replace the original L₁ normalization and 1+ELU activation with L₂ normalization and SiLU activation for query and key processing. Second, we add short convolution operations after the linear projections for queries, keys, and values. Third, we incorporate output normalization before the final projection.</p> <p>The complete processing pipeline now follows this structure:</p> <ul> <li>Query/Key: Linear → ShortConv → SiLU → L₂Norm</li> <li>Value: Linear → ShortConv → SiLU</li> <li>Beta: Linear → Sigmoid</li> <li>Output: Delta rule(query, key, value, beta) → RMSNorm → Linear</li> </ul> <p>Let’s examine why each of these modifications proves crucial for model performance.</p> <h3 id="normalization-on-queries-and-keys">Normalization on Queries and Keys</h3> <p>A crucial aspect of DeltaNet’s architecture is the normalization of key vectors. This isn’t just a technical detail - it’s fundamental to the model’s stability and effectiveness. Consider DeltaNet’s core equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top) + \mathbf{v}_t\mathbf{k}_t^\top\] <p>The stability of this recurrent system depends on the eigenvalues of its transition matrix \((\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top)\). This matrix has an elegant spectral structure:</p> <ul> <li>The eigenvalue in the direction of \(\mathbf{k}_t\) is \(1 - \beta_t\|\mathbf{k}_t\|^2\)</li> <li>All directions perpendicular to \(\mathbf{k}_t\) have eigenvalue 1</li> </ul> <p>For stable updates, we need all eigenvalues to have magnitude \(\leq 1\). Given \(0 \leq \beta_t \leq 1\), this requires \(\|\mathbf{k}_t\|^2 \leq 2\). While the original DeltaNet used L₁ normalization, we found L₂ normalization offers both better empirical performance and a more intuitive geometric interpretation: when \(\beta_t = 1\) and \(\|\mathbf{k}_t\|_2 = 1\), the matrix \(\mathbf{I} - \mathbf{k}_t\mathbf{k}_t^\top\) becomes a projection matrix that selectively erases information in the direction of \(\mathbf{k}_t\) while preserving all other directions.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/projection.png" alt="示例图片" style="width: 67%"/> </div> </div> <p>The projection matrix has an important geometric effect: when applied to any vector, it removes the component parallel to \(\mathbf{k}_t\) while preserving all orthogonal components. In the context of DeltaNet, this means each update “cleans up” the state by removing components that might interfere with the current key’s direction. This operation helps maintain cleaner separations between different key vectors over time, reducing the interference between stored patterns (or the retrieval error) that we discussed in the first post. This geometric property helps explain why L₂ normalization, which directly aligns with this projection interpretation, leads to better retrieval performance than L₁ normalization.</p> <p>We also find that applying L₂ normalization to queries improves model performance. This observation aligns with recent trends in self-attention architectures, where QK-normalization has emerged as an effective technique for stabilizing and enhancing attention mechanisms.</p> <p>Finally, we note a potential limitation in our current design: our transition matrices are constrained to have strictly positive eigenvalues. A recent insightful work<d-cite key="grazzi2024unlockingstatetrackinglinearrnns"></d-cite> demonstrates how this could limit the model’s state tracking capabilities. Fortunately, their proposed enhancement is remarkably simple - by adjusting our beta term to \(\beta_t = 2\beta_t\), we can allow for negative eigenvalues in our transition matrices. This one-line modification could meaningfully expand DeltaNet’s representational capabilities. We direct interested readers to the following discussion for a deeper analysis of this enhancement.</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">LLMs can now track states, finally matching this cat!<br/>And we prove it.<br/><br/>But how? 🧵👇<br/><br/>1/ Paper: <a href="https://t.co/aKvrqYtkWh">https://t.co/aKvrqYtkWh</a><br/>with <a href="https://twitter.com/julien_siems?ref_src=twsrc%5Etfw">@julien_siems</a> <a href="https://twitter.com/jkhfranke?ref_src=twsrc%5Etfw">@jkhfranke</a> <a href="https://twitter.com/ZelaArber?ref_src=twsrc%5Etfw">@ZelaArber</a>  <a href="https://twitter.com/FrankRHutter?ref_src=twsrc%5Etfw">@FrankRHutter</a>   <a href="https://twitter.com/MPontil?ref_src=twsrc%5Etfw">@MPontil</a> <a href="https://t.co/2OREoLkDyY">pic.twitter.com/2OREoLkDyY</a></p>&mdash; Riccardo Grazzi (@riccardograzzi) <a href="https://twitter.com/riccardograzzi/status/1860017064473428220?ref_src=twsrc%5Etfw">November 22, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h3 id="normalization-on-outputs">Normalization on Outputs</h3> <p>In standard linear attention, the output at each position is normalized by the sum of attention weights:</p> \[\mathbf{o}_t = \frac{(\sum_{i=1}^t \mathbf{v}_i \phi(\mathbf{k})_i^\top)\phi(\mathbf{q})_t}{\sum_{i=1}^t \phi(\mathbf{k})_i^\top \phi(\mathbf{q})_t}\] <p>where \(\phi\) is a positive feature map. However, a seminal analysis by Qin et al. <d-cite key="qin_devil_2022"></d-cite> demonstrates that this normalization term can lead to unbounded gradients and training instability. To address this issue, they propose removing the denominator and instead applying normalization to the output before the final projection. This architectural modification has since become standard practice, adopted by modern linear attention models including RetNet, GLA, Mamba2, and others.</p> <h3 id="activation-function-choice">Activation Function Choice</h3> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/silu.png" alt="示例图片" style="width: 50%"/> </div> </div> <p>While the original DeltaNet used 1+ELU activation, our experiments show that SiLU activation provides better performance, a finding aligned with recent architectures like Mamba2<d-cite key="mamba2"></d-cite>, xLSTM<d-cite key="beck_xlstm_2024"></d-cite>, and LightningAttention <d-cite key="Qin2024VariousLC"></d-cite>. This presents an interesting contrast with traditional linear attention models, which typically choose activation functions that ensure positive attention scores through positive feature maps (e.g., ReLU, 1+ELU, exponential). The success of allowing negative values parallels findings from Differential Transformers <d-cite key="ye2024differentialtransformer"></d-cite>, suggesting that restricting attention scores to be strictly positive may be unnecessarily limiting.</p> <h3 id="short-convolution">Short Convolution</h3> <p>Short convolution<d-cite key="Poli2023HyenaHT"></d-cite>, i.e., depthwise separable Conv1D with small kernel window size as small as 4, has emerged as a crucial component in recent subquadratic attention models, appearing in various forms across architectures like Mamba, xLSTM<d-cite key="beck_xlstm_2024"></d-cite>, and MetaLA<d-cite key="chou2024metala"></d-cite>. This generalizes the previously proposed “shift-SSM” in H3<d-cite key="h3"></d-cite> and has a close relationship to “token-shift” in RWKV<d-cite key="peng-etal-2023-rwkv"></d-cite>.</p> <p>Regarding why short convolution is effective, we believe it is because it provides a “shortcut” to form induction heads <d-cite key="olsson2022context"></d-cite> within a single layer, which is beneficial for in-context learning, and has been found even useful in softmax attention under large scale <d-cite key="Xu2024KVSA"></d-cite>.</p> <h3 id="experimental-results">Experimental Results</h3> <p>With the parallel algorithm in hand, and with the architecture above, we are now ready to scale up DeltaNet to standard language modeling settings. Our evaluation spans three key metrics: language modeling (WikiText perplexity), common-sense reasoning (averaged across LAMBADA, PiQA, HellaSwag, WinoGrande, ARC-easy, and ARC-challenge), and in-context retrieval (averaged across FDA, SWDE, and SQuAD).</p> <p>Regarding state size across architectures (H denotes number of layers, d denotes model dimension):</p> <table> <thead> <tr> <th>Architecture</th> <th>State Expansion</th> <th>Total State Size</th> <th>Implementation Details</th> </tr> </thead> <tbody> <tr> <td><strong>Mamba</strong></td> <td>16x</td> <td>64Hd</td> <td>Expands value projections to 2d and uses 16x expansion ratio; doubles effective state size by replacing FFN with Mamba layers</td> </tr> <tr> <td><strong>RetNet</strong></td> <td>512x</td> <td>512Hd</td> <td>Expands value projections to 2d; maintains fixed 256-dimensional query/key heads</td> </tr> <tr> <td><strong>GLA</strong></td> <td>256x</td> <td>256Hd</td> <td>Uses half-sized query/key heads relative to value heads; maintains 4d² parameters per layer</td> </tr> <tr> <td><strong>DeltaNet</strong></td> <td>128x</td> <td>128Hd</td> <td>Employs consistent 128-dimensional heads throughout the architecture</td> </tr> </tbody> </table> <h4 id="main-results-340m15b-token">Main Results (340M+15B token)</h4> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Avg. Common-sense ↑</th> <th>Avg. Retrieval ↑</th> <th>State Size</th> </tr> </thead> <tbody> <tr> <td>Transformer++</td> <td>28.39</td> <td>41.2</td> <td>28.6</td> <td>N/A</td> </tr> <tr> <td>RetNet (w/o conv)</td> <td>32.33</td> <td>41.0</td> <td>14.6</td> <td>512x</td> </tr> <tr> <td>Mamba (w. conv)</td> <td>28.39</td> <td>41.8</td> <td>12.5</td> <td>64x</td> </tr> <tr> <td>GLA (w/o conv)</td> <td>28.65</td> <td>41.5</td> <td>18.0</td> <td>128x</td> </tr> <tr> <td>DeltaNet (w. conv)</td> <td>28.24</td> <td>42.1</td> <td>22.7</td> <td>128x</td> </tr> </tbody> </table> <p>DeltaNet achieves competitive performance across all metrics while maintaining reasonable state size requirements. Notably, it shows particular strength in retrieval tasks, supporting our hypothesis that its delta rule mechanism provides effective in-context retrieval capabilities.</p> <h4 id="ablation-study-340m15b-token">Ablation Study (340M+15B token)</h4> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Common-sense ↑</th> <th>Retrieval ↑</th> </tr> </thead> <tbody> <tr> <td>DeltaNet (full)</td> <td>28.24</td> <td>42.1</td> <td>22.7</td> </tr> <tr> <td>- w/o short conv</td> <td>29.08</td> <td>41.4</td> <td>18.6</td> </tr> <tr> <td>- w. \(L_1\)-norm + 1+ELU</td> <td>31.12</td> <td>40.1</td> <td>11.5</td> </tr> <tr> <td>- w. \(L_2\)-norm + 1+ELU</td> <td>28.03</td> <td>42.1</td> <td>21.8</td> </tr> <tr> <td>- w. \(L_2\)-norm + ReLU</td> <td>28.75</td> <td>40.9</td> <td>21.0</td> </tr> </tbody> </table> <p>Our ablation studies highlight several important findings about DeltaNet’s architecture. Most significantly, retrieval performance shows strong sensitivity to the choice of normalization - \(L_2\) normalization substantially outperforms \(L_1\) normalization, supporting our theoretical analysis about projection properties. Short convolution also emerges as a crucial component, demonstrating that effective position-based addressing meaningfully complements DeltaNet’s content-based mechanism for retrieval tasks. The choice of activation function, while still relevant, shows more modest effects; SiLU provides incremental improvements over ReLU and 1+ELU, but its impact is less pronounced than either normalization or short convolution.</p> <h2 id="hybrid-models-combining-deltanet-with-attention">Hybrid Models: Combining DeltaNet with Attention</h2> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/hybrid.png" alt="示例图片" style="width: 99%"/> <figcaption style="margin-top: 10px; color: #666;"> Left: Hybrid sliding window attention and DeltaNet model. Right: Hybrid global attention and DeltaNet model. </figcaption> </div> </div> <p>While DeltaNet’s delta rule mechanism shows promise for retrieval tasks, it still faces a fundamental limitation common to all RNN architectures: fixed state size. This constraint creates an inherent ceiling for retrieval performance, regardless of the choice of update rule<d-cite key="wen_rnns_2024"></d-cite>. To overcome this limitation, we explore hybrid architectures that strategically combine DeltaNet with attention mechanisms.</p> <p>Our first approach integrates sliding window attention with DeltaNet in an interleaving pattern, following recent architectures like Griffin<d-cite key="de_griffin_2024"></d-cite> and Samba<d-cite key="ren2024samba"></d-cite>. While this hybrid maintains subquadratic complexity due to the fixed window size, it inherits similar theoretical constraints as pure RNN models. As demonstrated in Griffin<d-cite key="de_griffin_2024"></d-cite>, the fixed context window can limit the retrieval of information beyond its scope.</p> <p>This limitation led us to our second approach: augmenting DeltaNet with global attention. Rather than replacing many DeltaNet layers with attention, which would significantly impact inference efficiency, we choose to place just two global attention layers - one in the second layer and another at layer N/2-1, following H3<d-cite key="h3"></d-cite>. Though this technically makes the model no longer subquadratic, the sparing use of attention layers substantially reduces KV cache requirements compared to full Transformer models.</p> <p>Results at the 340M parameter scale demonstrate the effectiveness of these hybrid approaches:</p> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Avg. Common-sense ↑</th> <th>Avg. Retrieval ↑</th> </tr> </thead> <tbody> <tr> <td>Transformer++</td> <td>28.39</td> <td>41.2</td> <td>28.6</td> </tr> <tr> <td>DeltaNet</td> <td>28.24</td> <td>42.1</td> <td>22.7</td> </tr> <tr> <td>+ Sliding Attn</td> <td>27.06</td> <td>42.1</td> <td>30.2</td> </tr> <tr> <td>+ Global Attn</td> <td>27.51</td> <td>42.1</td> <td>32.7</td> </tr> </tbody> </table> <p>We then scaled our experiments to 1.3B parameters, training for 100B tokens on SlimPajama. The results reinforce our findings:</p> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Avg. Common-sense ↑</th> <th>Avg. Retrieval ↑</th> </tr> </thead> <tbody> <tr> <td>Transformer++</td> <td>16.85</td> <td>50.9</td> <td>41.8</td> </tr> <tr> <td>DeltaNet</td> <td>16.87</td> <td>51.6</td> <td>34.7</td> </tr> <tr> <td>+ Sliding Attn</td> <td>16.56</td> <td>52.1</td> <td>39.6</td> </tr> <tr> <td>+ Global Attn</td> <td>16.55</td> <td>51.8</td> <td>47.9</td> </tr> </tbody> </table> <p>While sliding window attention provides substantial gains, it cannot fully match Transformer-level retrieval performance in larger scale. However, the addition of just two global attention layers<d-footnote>Recent work demonstrates that using global attention in only a small portion (~10%) of total layers can be highly effective for model performance <d-cite key="DBLP:journals/corr/abs-2406-07887,DBLP:journals/corr/abs-2403-19887"></d-cite>.</d-footnote> yields remarkable results, surpassing even the Transformer baseline in retrieval tasks.</p> <p>Finally, we evaluated a 3B parameter model trained on 1T tokens following the PowerLM-3B setup <d-cite key="Shen2024PowerSA"></d-cite>. These results place DeltaNet as a strong performer among RNN architectures while slightly trailing transformer-based models:</p> <table> <thead> <tr> <th>Model</th> <th>ARC</th> <th>HellaSwag</th> <th>OBQA</th> <th>PIQA</th> <th>WinoGrande</th> <th>MMLU</th> <th>Average</th> </tr> </thead> <tbody> <tr> <td>Llama-3.2-3B</td> <td>59.1</td> <td>73.6</td> <td>43.4</td> <td>77.5</td> <td>69.2</td> <td>54.1</td> <td>62.8</td> </tr> <tr> <td>PowerLM-3B</td> <td>60.5</td> <td>74.6</td> <td>43.6</td> <td>79.9</td> <td>70.0</td> <td>45.0</td> <td>62.3</td> </tr> <tr> <td>DeltaNet-3B</td> <td>60.4</td> <td>72.8</td> <td>41.0</td> <td>78.5</td> <td>65.7</td> <td>40.7</td> <td>59.8</td> </tr> <tr> <td>RecurrentGemma-2B</td> <td>57.0</td> <td>71.1</td> <td>42.0</td> <td>78.2</td> <td>67.6</td> <td>31.8</td> <td>57.9</td> </tr> <tr> <td>RWKV-6-3B</td> <td>49.5</td> <td>68.6</td> <td>40.6</td> <td>76.8</td> <td>65.4</td> <td>28.4</td> <td>54.9</td> </tr> <tr> <td>Mamba-2.7B</td> <td>50.3</td> <td>65.3</td> <td>39.4</td> <td>75.8</td> <td>63.1</td> <td>26.1</td> <td>53.3</td> </tr> </tbody> </table> <p>The results demonstrate DeltaNet’s effectiveness across scales, though there remains a small gap compared to transformer architectures at larger sizes. We are currently exploring larger hybrid models combining DeltaNet with attention mechanisms - stay tuned for updates!</p>]]></content><author><name>Songlin Yang</name></author><summary type="html"><![CDATA[Modernize DeltaNet neural architecture]]></summary></entry></feed>