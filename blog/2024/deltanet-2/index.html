<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeltaNet Explained (Part II) | Songlin Yang</title> <meta name="author" content="Songlin Yang"> <meta name="description" content="An algorithm that parallelizes DeltaNet computation across the sequence length dimension"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/maomao.jpeg?4c57e11efcfd0cc3186dbb6930d90ab5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sustcsonglin.github.io/blog/2024/deltanet-2/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "DeltaNet Explained (Part II)",
      "description": "An algorithm that parallelizes DeltaNet computation across the sequence length dimension",
      "published": "December 3, 2024",
      "authors": [
        {
          "author": "Songlin Yang",
          "authorURL": "https://sustcsonglin.github.io/",
          "affiliations": [
            {
              "name": "MIT CSAIL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Songlin </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>DeltaNet Explained (Part II)</h1> <p>An algorithm that parallelizes DeltaNet computation across the sequence length dimension</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#parallel-scan-for-deltanet-a-failed-attempt">Parallel Scan for DeltaNet: A Failed Attempt</a></div> <ul> <li><a href="#from-delta-updates-to-matrix-multiplication-form">From Delta Updates to Matrix Multiplication Form</a></li> <li><a href="#defining-the-associative-operator">Defining the Associative Operator</a></li> <li><a href="#parallel-scan-for-deltanet">Parallel Scan for DeltaNet</a></li> <li><a href="#what-s-wrong-with-parallel-scan-for-deltanet">What's Wrong with Parallel Scan for DeltaNet?</a></li> </ul> <div><a href="#a-chunkwise-algorithm-for-deltanet">A Chunkwise Algorithm for DeltaNet</a></div> <ul> <li><a href="#chunkwise-parallel-form-for-linear-attention">Chunkwise Parallel Form for Linear Attention</a></li> <li><a href="#wy-representation-for-deltanet">WY representation for DeltaNet</a></li> <li><a href="#chunkwise-parallel-form-for-deltanet">Chunkwise Parallel Form for DeltaNet</a></li> <li><a href="#ut-transform-through-the-lens-of-graph-theory">UT Transform Through the Lens of Graph Theory</a></li> <li><a href="#speed-comparison">Speed comparison</a></li> </ul> </nav> </d-contents> <p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484" rel="external nofollow noopener" target="_blank">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></strong> (w/ <a href="https://berlino.github.io/" rel="external nofollow noopener" target="_blank">Bailin Wang</a>, <a href="https://yzhang.site/" rel="external nofollow noopener" target="_blank">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/" rel="external nofollow noopener" target="_blank">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/" rel="external nofollow noopener" target="_blank">Yoon Kim</a>). <strong>You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py" rel="external nofollow noopener" target="_blank">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf" rel="external nofollow noopener" target="_blank">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="#">Part II - The Algorithm</a></li> <li><a href="/blog/2024/deltanet-3/">Part III - The Neural Architecture</a></li> </ol> <h2 id="parallel-scan-for-deltanet-a-failed-attempt">Parallel Scan for DeltaNet: A Failed Attempt</h2> <p>Ok, we’ve seen in the previous section that DeltaNet does really well on these diagnostic synthetic tasks. So now we just need to scale it up to modern LMs, right? Well, it turns out it’s not that simple. In particular, the original DeltaNet treated DeltaNet as a pure RNN which required O(L) sequential steps, which is inefficient on modern hardware such as GPUs with massive parallel processing capabilities. We thus seek strategies to parallelize DeltaNet across sequence length to enable hardware-efficient training. In this post, we first discuss parallel scan as a interesting-but-impractical strategy for parallelizing DeltaNet. We then give another algorithm for parallelization that is more efficient in practice.</p> <h3 id="from-delta-updates-to-matrix-multiplication-form">From Delta Updates to Matrix Multiplication Form</h3> <p>Let’s start with DeltaNet’s original state update equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top\] <p>To transform this into a matrix multiplication form, let’s expand the equation step by step:</p> \[\begin{align*} \mathbf{S}_{t} &amp;= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \end{align*}\] <p>For simplicity, let’s denote:</p> <ul> <li>\(\mathbf{M}_t = \mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal\) as our transition matrix</li> <li>\(\mathbf{X}_t = \beta_t \mathbf{v}_t \mathbf{k}_t^\top\) as our update term</li> </ul> <p>Then our update becomes:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1}\mathbf{M}_t + \mathbf{X}_t \in \mathbb{R}^{d\times d}\] <h3 id="defining-the-associative-operator">Defining the Associative Operator</h3> <p>This form matches exactly with the first-order recurrence shown in equation (1.5) from <em>Prefix Sums and Their Applications</em><d-cite key="Blelloch1990PrefixSA"></d-cite>, where matrix multiplication (⊗) and matrix addition (⊕) serve as our binary operators. Both operators satisfy the required properties:</p> <ol> <li>Matrix addition is associative: \((A + B) + C = A + (B + C)\)</li> <li>Matrix multiplication is associative: \((AB)C = A(BC)\)</li> <li>Matrix multiplication distributes over addition: \(A(B + C) = AB + AC\)</li> </ol> <p>Following the framework, we define our state pairs as:</p> \[c_t = [\mathbf{M}_t, \mathbf{X}_t] = [\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal, \beta_t \mathbf{v}_t \mathbf{k}_t^\top]\] <p>And our associative operator • that combines these pairs:</p> \[c_i \bullet c_j = [\mathbf{M}_i\mathbf{M}_j, \mathbf{M}_j\mathbf{X}_i + \mathbf{X}_j]\] <p>This operator definition preserves the temporal dependencies of our updates - when we combine two steps, the earlier update term \(\mathbf{X}_i\) must be transformed by the later transition matrix \(\mathbf{M}_j\), while the later update term \(\mathbf{X}_j\) remains unchanged.</p> <h3 id="parallel-scan-for-deltanet">Parallel Scan for DeltaNet</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/scan.png" alt="示例图片" style="width: 99%"> </div> </div> <p>With this associative operator, we can use parallel scan to compute all states in parallel. The algorithm works in two phases:</p> <h5 id="sweep-down-phase">Sweep-Down Phase</h5> <p>First, we compute partial results in parallel by combining adjacent pairs:</p> <p>For steps 0 and 1, we compute:</p> \[c_1 = c_0 \bullet c_1 = [\mathbf{M}_0\mathbf{M}_1, \mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1]\] <p>Similarly for steps 2 and 3:</p> \[c_3 = c_2 \bullet c_3 = [\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <p>Then combine these results:</p> \[c_{1:3} = c_{1} \bullet c_{3} = [\mathbf{M}_0\mathbf{M}_1\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_2\mathbf{M}_3(\mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1) + \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <h5 id="sweep-up-phase">Sweep-Up Phase</h5> <p>In this phase, we use our partial results to compute intermediate states:</p> \[c_2 = c_1 \bullet c_2 = [\mathbf{M}_1\mathbf{M}_2, \mathbf{M}_2\mathbf{X}_1 + \mathbf{X}_2]\] <p>This parallelization transforms DeltaNet’s sequential state updates into an efficient parallel computation, reducing the sequential dependency chain from \(\mathbf{O}(L)\) to \(\mathcal{O}(\log L)\) steps while maintaining mathematical equivalence.</p> <h3 id="whats-wrong-with-parallel-scan-for-deltanet">What’s Wrong with Parallel Scan for DeltaNet?</h3> <p>Despite parallelizability, parallel scan for DeltaNet faces two major challenges: computational complexity and memory requirements.</p> <p>The first issue lies in the <strong>time complexity</strong>. For DeltaNet, parallel scan yields \(\mathcal{O}(L\log L d^3)\) complexity due to the cubic cost of matrix multiplication when treating \(\mathbf{M}_t\) as dense matrices. At first glance, we might think we can leverage the identity-plus-low-rank structure of \(\mathbf{M}_t\) for acceleration. Let’s work through this carefully.</p> <p>When multiplying two adjacent matrices, we get:</p> \[\begin{align*} (\mathbf{I}-\beta_0 \mathbf{k}_0 \mathbf{k}_0^\top)(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) &amp;= \mathbf{I}(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) \\ &amp;= (\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 \mathbf{k}_0^\top \mathbf{k}_1 \mathbf{k}_1^\top \\ &amp;= \mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top \end{align*}\] <p>This computation reduces the complexity from \(\mathcal{O}(d^3)\) to \(\mathcal{O}(d^2)\) by leveraging the identity-plus-low-rank structure - we only need to compute vector inner products \((\mathbf{k}_0^\top \mathbf{k}_1)\) and outer products between vectors. Similarly for the next pair:</p> \[\begin{align*} (\mathbf{I}-\beta_2 \mathbf{k}_2 \mathbf{k}_2^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top) &amp;= \mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top \end{align*}\] <p>When we try to combine these results to compute larger spans like \(c_{1:4}\), the multiplication becomes increasingly complex. We need to multiply:</p> \[(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top)\] <p>Each term in the first bracket must multiply with each term in the second bracket. While each matrix is initially a sum of \(O(1)\) rank-1 terms, this multiplication leads to a quadratic growth in the number of terms. After \(\log L\) levels of parallel scan, we end up with \(O(L^{\log c})\) terms, where \(c\) is the initial number of terms per matrix. This exponential growth in the number of terms, despite each being rank-1, makes maintaining the explicit structure impractical. Therefore, treating these as dense matrices with \(O(d^3L \log L)\) complexity becomes a more reasonable approach, especially considering the efficiency of dense matrix operations on modern hardware. This explains why parallel scan, while theoretically appealing, faces significant practical challenges for DeltaNet computation.</p> <p>The second major issue is <strong>space complexity</strong>. Parallel scan requires materializing all intermediate d×d matrices at each step to high-bandwidth memory (HBM). For linear RNNs with matrix-valued states, this materialization becomes prohibitively expensive (\(\mathcal{O}(Ld^2)\)). While recurrent computation can avoid such materialization <d-footnote> See section 3.3.1 of Katharopoulos et al.<d-cite key="katharopoulos2020transformers"></d-cite> for more details.</d-footnote>, parallel scan offers no apparent workaround unless all states fit into SRAM, as implemented in Mamba’s hardware-aware selective scan algorithm that eliminates the need for materialization. However, this approach imposes limitations on state size - too large a state leads to out-of-shared-memory issues. Given that I/O costs dominate this computation, parallel scan may become undesirable in practice. As noted in recent discussions:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Dear parallel-scan people.<br><br>Q(t) and K(t) are 1xD and V(t) is 1xC, with D and T large, and I want to compute<br><br>Y(t) = Q(t)M(t)<br><br>with<br><br>M(0)=0<br>M(t+1) = M(t) + K(t)^T V(t)<br><br>A naive non-parallel scan approach is O(T) in time but I do not have to store any DxD<br><br>1/2</p>— François Fleuret (@francoisfleuret) <a href="https://twitter.com/francoisfleuret/status/1793016689589625263?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">May 21, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">The key idea is to only materialize chunk-level hidden states, using matmuls to calculate outputs based on the query, key, and value matrices and chunk-level hidden states. This method avoids materializing the hidden state for every single timestep.</p>— Songlin Yang (@SonglinYang4) <a href="https://twitter.com/SonglinYang4/status/1793029555277697379?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">May 21, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Here I previously discussed the chunkwise algorithm - another type of associative scan that offers improved memory efficiency and better utilization of tensor cores by enabling more matrix multiplication operations (for a detailed analysis, see <d-cite key="yang_gated_2023"></d-cite>). Given these advantages, developing a chunkwise training algorithm for DeltaNet that maintains quadratic complexity with respect to \(d\) while preserving memory efficiency would be highly valuable.</p> <h2 id="a-chunkwise-algorithm-for-deltanet">A Chunkwise Algorithm for DeltaNet</h2> <h3 id="chunkwise-parallel-form-for-linear-attention">Chunkwise Parallel Form for Linear Attention</h3> <p>Linear attention’s efficiency stems from its ability to maintain a compact representation of the state using vectors rather than materializing full matrices. This is possible because a sum of outer products can be rewritten as matrix multiplication:</p> \[\begin{align*} \sum_{i=1}^t \mathbf{v}_i \mathbf{k}_i^\top &amp;= \mathbf{V}_t\mathbf{K}_t^\top \\ \text{where } \mathbf{V}_t &amp;= [\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_t] \\ \mathbf{K}_t &amp;= [\mathbf{k}_1, \mathbf{k}_2, ..., \mathbf{k}_t] \end{align*}\] <p>This matrix multiplication form is highly optimized on modern GPUs with tensor cores. Leveraging this property, instead of storing all intermediate hidden states, we can store states only at regular intervals of size \(C\) as checkpoints. This gives us states \(\mathbf{S}_{0}, \mathbf{S}_{C}, \mathbf{S}_{2C}, ..., \mathbf{S}_{(n-1)C}\) where \(n = \lceil L/C \rceil\).</p> <p>Denoting \(\mathbf{S}_{[i]} := \mathbf{S}_{iC} \in \mathbb{R}^{d \times d}\); \(\square_{[i]} = \square_{iC+1:(i+1)C} \in \mathbb{R}^{C \times d}\) for \(\square \in \{\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}\}\); \(\square_{[i]}^r = \square_{iC+r}\) for \(\square \in \{\mathbf{q}, \mathbf{k}, \mathbf{v}, \mathbf{o}, \mathbf{S}\}\). For any position r within chunk i, we can compute:</p> \[\begin{align*} \mathbf{S}_{[i]}^r &amp;= \mathbf{S}_{[i]} + \sum_{t=1}^{r} \mathbf{v}_{[i]}^t \mathbf{k}_{[i]}^{t\top} \\ \mathbf{o}_{[i]}^r &amp;= \mathbf{S}_{[i]}^r \mathbf{q}_{[i]}^r = \mathbf{S}_{[i]}\mathbf{q}_{[i]}^r + \sum_{t=1}^{r} \mathbf{v}_{[i]}^t (\mathbf{k}^{t\top}_{[i]} \mathbf{q}_{[i]}^r) \end{align*}\] <p>and in matrix form,</p> \[\begin{align*} \mathbf{S}_{[t+1]} &amp;= \mathbf{S}_{[t]} + \mathbf{V}_{[t]}^\top \mathbf{K}_{[t]} &amp;&amp; \in \mathbb{R}^{d\times d} \\ \mathbf{O}_{[t]} &amp;= \mathbf{Q}_{[t]} \mathbf{S}_{[t]}^\top + (\mathbf{Q}_{[t]}\mathbf{K}_{[t]}^\top \odot \mathbf{M}) \mathbf{V}_{[t]} &amp;&amp; \in \mathbb{R}^{C\times d} \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/chunk-linear-attn.png" alt="示例图片" style="width: 99%"> <figcaption style="margin-top: 10px; color: #666; text-align: center;"> A visual representation of the chunkwise algorithm for linear attention </figcaption> </div> </div> <p>This chunkwise formulation enables efficient hardware utilization by leveraging tensor cores when the chunk size C is a multiple of 16, as implemented in our open-source library <strong>flash-linear-attention</strong><d-cite key="yang_fla_2024"></d-cite>.</p> <h3 id="wy-representation-for-deltanet">WY representation for DeltaNet</h3> <p>However, as we saw in our failed attempt above, the cumulative product of DeltaNet’s transition matrices seemed to resist such compact representation, apparently requiring us to store numerous intermediate results. Fortunately, there’s a solution: DeltaNet’s transition matrices closely resemble <a href="https://en.wikipedia.org/wiki/Householder_transformation" rel="external nofollow noopener" target="_blank">Householder matrices</a> (when \(\beta_t\)=2), and there exists an elegant compact representation for their cumulative product.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/householder.png" alt="示例图片" style="width: 49%"> <figcaption style="margin-top: 10px; color: #666;"> A visual representation of the Householder reflector transformation </figcaption> </div> </div> <p>This representation, known as the WY representation, was introduced in a seminal 1985 paper<d-cite key="bischof_wy_1985"></d-cite>. Using DeltaNet’s notation, the cumulative product can be written as:</p> \[\prod_{i=1}^{t} (\mathbf{I} - \beta_i \mathbf{k}_i \mathbf{k}_i^\top) = \mathbf{I} - \sum_{i=1}^t \mathbf{w}_i\mathbf{k}_i^\top\] <p>We can prove this by mathematical induction. Let’s define \(\mathbf{P}_n = \prod_{t=1}^n(\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\top)\). For n=1, the equation clearly holds. Assuming it holds for n-1, we can prove it for n:</p> \[\begin{align*} \mathbf{P}_n &amp;= \mathbf{P}_{n-1} (\mathbf{I} - \beta_n \mathbf{k}_n \mathbf{k}_n^\top) \\ &amp;= (\mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top)(\mathbf{I} - \beta_n \mathbf{k}_n \mathbf{k}_n^\top) \\ &amp;= \mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top - \beta_n \mathbf{k}_n \mathbf{k}_n^\top + (\sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top) \beta_n \mathbf{k}_n \mathbf{k}_n^\top \\ &amp;= \mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top - \underbrace{\left(\beta_n \mathbf{k}_n - \beta_n \sum_{t=1}^{n-1} \left(\mathbf{w}_t (\mathbf{k}_t^\top\mathbf{k}_n)\right) \right)}_{\mathbf{w}_n}\mathbf{k}_n^\top \\ &amp;= \mathbf{I} - \sum_{t=1}^n \mathbf{w}_t\mathbf{k}_t^\top \end{align*}\] <p>This proof not only establishes the correctness of the representation but also provides a constructive way to compute the \(\mathbf{w}\) vectors!</p> <p>Similarly, we can show \(\mathbf{S}_n = \sum_{t=1}^{n} \mathbf{u}_t \mathbf{k}_t^\top\) by induction:</p> \[\begin{align*} \mathbf{S}_n &amp;= \mathbf{S}_{n-1} (\mathbf{I} - \beta_n \mathbf{k}_n\mathbf{k}_n^\top) + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \left(\sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top\right) (\mathbf{I} - \beta_n \mathbf{k}_n\mathbf{k}_n^\top) + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top - \left(\sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top\right) \beta_n \mathbf{k}_n \mathbf{k}_n^\top + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top + \underbrace{\left(\beta_n \mathbf{v}_n - \beta_n\sum_{t=1}^{n-1} \mathbf{u}_t \left(\mathbf{k}_t^\top \mathbf{k}_n \right) \right)}_{\mathbf{u}_n} \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n} \mathbf{u}_t \mathbf{k}_t^\top \end{align*}\] <p>Looking at this sum-of-outer-products structure, we can see it closely resembles linear attention’s update form. This similarity suggests a path toward developing a novel parallel algorithm!</p> <h3 id="chunkwise-parallel-form-for-deltanet">Chunkwise Parallel Form for DeltaNet</h3> <p>First, let’s unroll the recurrence of DeltaNet:</p> \[\begin{align*} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\top) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \sum_{i=1}^t \beta_i (\mathbf{v}_i \mathbf{k}_i^\top) \left(\prod_{j=i+1}^t (\mathbf{I} - \beta_j \mathbf{k}_j \mathbf{k}_j^\top)\right) \end{align*}\] <p>Similar to linear attention, we can use checkpointing to store states at regular intervals of size C. For any position r within chunk i, we have:</p> \[\begin{align*} \mathbf{S}_{[i]}^r &amp;= \mathbf{S}_{[i]} \underbrace{\prod_{t=1}^{r} (\mathbf{I}-\beta_{[i]}^t\mathbf{k}_{[i]}^t\mathbf{k}_{[i]}^{t\top})}_{\text{chunk-local cumprod: } \mathbf{P}_{[i]}^r} + \underbrace{\sum_{t=1}^{r} (\beta_{[i]}^t \mathbf{v}_{[i]}^t \mathbf{k}_{[i]}^{t\top} \prod_{s=t+1}^{r} (\mathbf{I}-\beta_{[i]}^s\mathbf{k}_{[i]}^s\mathbf{k}_{[i]}^{s\top}))}_{\text{chunk-local state or cumprodsum: }\mathbf{H}_{[i]}^r} \\ &amp;= \mathbf{S}_{[i]} (\mathbf{I} - \sum_{t=1}^r\mathbf{w}_{[i]}^t\mathbf{k}_{[i]}^{t\top}) + \sum_{t=1}^r \mathbf{u}_{[i]}^t \mathbf{k}_{[i]}^{t\top}\\ \end{align*}\] <p>where \(\mathbf{w}_{[i]}^t\) and \(\mathbf{u}_{[i]}^t\) are computed using the WY representation, but starting from the first position of each chunk rather than the beginning of the sequence, enabling parallel computation across chunks.</p> \[\mathbf{w}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{k}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{w}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] \[\mathbf{u}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{v}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] <p>And for output computation,</p> \[\begin{align*} \mathbf{o}_{[i]}^r &amp;= \mathbf{S}_{[i]}^r \mathbf{q}_{[i]}^r \\ &amp;= \mathbf{S}_{[i]} \mathbf{q}_{[i]}^r - \sum_{t=1}^r \mathbf{S}_{[i]}\mathbf{w}_{[i]}^t(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) + \sum_{t=1}^r \mathbf{u}_{[i]}^t(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) \\ &amp;= \mathbf{S}_{[i]} \mathbf{q}_{[i]}^r + \sum_{t=1}^r (\mathbf{u}_{[i]}^t - \mathbf{S}_{[i]}\mathbf{w}_{[i]}^t)(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) \end{align*}\] <p>Together, in matrix-multiplication form,</p> \[\begin{align*} \mathbf{S}_{[i+1]} &amp;= \mathbf{S}_{[i]} (\mathbf{I}-\mathbf{W}_{[i]}^\top \mathbf{K}_{[i]}) + \mathbf{U}_{[i]}^\top \mathbf{K}_{[i]} \\ &amp;= \mathbf{S}_{[i]} + \left(\mathbf{U}_{[i]} - \mathbf{W}_{[i]}\mathbf{S}_{[i]}^\top\right)^\top \mathbf{K}_{[i]} &amp;&amp; \in \mathbb{R}^{d\times d} \\ \mathbf{O}_{[i]} &amp;= \mathbf{Q}_{[i]} \mathbf{S}_{[i]}^\top + (\mathbf{Q}_{[i]} \mathbf{K}_{[i]}^\top \odot \mathbf{M}) \left(\mathbf{U}_{[i]} - \mathbf{W}_{[i]} \mathbf{S}_{[i]}^\top\right) &amp;&amp; \in \mathbb{R}^{C\times d} \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-chunk.png" alt="示例图片" style="width: 99%"> <figcaption style="margin-top: 10px; color: #666; text-align: center;"> A visual representation of the chunkwise algorithm for DeltaNet </figcaption> </div> </div> <h3 id="ut-transform-through-the-lens-of-graph-theory">UT Transform Through the Lens of Graph Theory</h3> <p>The chunkwise parallel form transforms most of DeltaNet’s operations into efficient matrix multiplications similar to linear attention. However, there’s a key computational bottleneck: the recursive construction of update vectors \(\mathbf{U}_{[i]}\) and \(\mathbf{W}_{[i]}\). This motivates the need for the UT transform<d-cite key="Joffrain2006AccumulatingHT"></d-cite> - to restructure this recursive computation into a form that can leverage efficient matrix multiplication. Let’s understand this through graph theory.</p> <p>In graph theory, for a weighted directed graph, the adjacency matrix \(\mathbf{A}\) captures direct connections - entry \(\mathbf{A}[i,j]\) represents the edge weight from node \(j\) to node \(i\). When we compute \((\mathbf{I} - \mathbf{A})^{-1}\), each entry \([i,j]\) gives the sum of weights of all possible paths from \(j\) to \(i\).</p> <p>Looking at our recursive update equations:</p> \[\mathbf{w}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{k}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{w}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] \[\mathbf{u}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{v}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] <p>These form a weighted directed graph where:</p> <ul> <li>Nodes represent sequence positions</li> <li>Directed edges connect position \(i\) to \(r\) where \(i &lt; r\) (causal dependency)</li> <li>Edge weights \(-\beta_{[t]}^r\mathbf{k}_{[t]}^{i\top}\mathbf{k}_{[t]}^r\) encode interactions through key similarity and learning rate</li> </ul> <p>This graph structure can be represented by the adjacency matrix, which could be computed efficiently:</p> \[\mathbf{A}_{[t]} = \operatorname{tril}(-\operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{K}_{[t]} \mathbf{K}_{[t]}^\top,-1)\] <p>Since \(\mathbf{A}_{[t]}\) is strictly lower triangular, \((\mathbf{I} - \mathbf{A}_{[t]})\) is also lower triangular with ones on the diagonal. This special structure allows us to efficiently compute its inverse through forward substitution:</p> \[\mathbf{T}_{[t]} = \left(\mathbf{I} - \mathbf{A}_{[t]}\right)^{-1}\] <p>This avoids the need for general matrix inversion, making the computation much more efficient. After obtaining \(\mathbf{T}_{[t]}\), which captures all accumulated influence paths between positions, we proceed with the final multiplication:</p> \[\mathbf{W}_{[t]} = \mathbf{T}_{[t]} \operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{K}_{[t]}, \quad \mathbf{U}_{[t]}=\mathbf{T}_{[t]}\operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{V}_{[t]}\] <p>Applies these accumulated influences to compute our updates in a hardware-efficient form.</p> <h3 id="speed-comparison">Speed comparison</h3> <p>We implemented both the recurrent and chunkwise parallel versions of DeltaNet using Triton. Our experiments compare their performance across different sequence lengths (\(L\)) and head dimensions (\(d_{\text{head}}\)), with a fixed model dimension \(d=2048\). To ensure fair comparison across configurations, we kept the total sequence elements constant at 16,384 by adjusting batch sizes accordingly.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/speedup.png" alt="示例图片" style="width: 60%"> </div> </div> <p>As we can see in the figure above, our chunkwise parallel approach consistently outperforms the recurrent baseline. More importantly, this performance advantage grows more pronounced under two key conditions: as sequences get longer and as head dimensions increase. To understand why, let’s examine two fundamental limitations of recurrent implementations that our approach addresses.</p> <p>The first limitation concerns parallelism strategy. Recurrent implementations process sequences step-by-step, relying primarily on two sources of parallelism to keep GPU cores busy: the batch dimension (processing multiple sequences simultaneously) and the head dimension (computing multiple attention heads in parallel). While this strategy worked well with moderate sequence lengths and larger batch sizes, it faces challenges in modern training scenarios. Today’s models increasingly work with longer sequences or larger model parameters, often necessitating smaller batch sizes for memory efficiency. This shift was notably highlighted in the FlashAttention2 paper<d-cite key="flashattention2"></d-cite>, which identified sequence-level parallelism as crucial for training. Without the ability to parallelize across the sequence dimension, recurrent implementations hit a fundamental bottleneck: when the product of batch size and number of attention heads is small, they can’t provide enough parallel work to keep modern GPUs fully utilized. This results in low occupancy of Streaming Multiprocessors (SMs) and suboptimal speed performance.</p> <p>The second limitation relates to hardware utilization. Modern GPUs include specialized tensor cores designed to accelerate matrix multiplication operations, offering up to 16x speedup for half-precision computations compared to other operations with equivalent FLOP counts. Recurrent implementations, despite requiring fewer total FLOPs, struggle to effectively leverage these hardware accelerators. This becomes particularly problematic with larger head dimensions, which are often necessary for tasks requiring substantial memory capacity (like in-context retrieval). Our chunkwise implementation, in contrast, restructures the computation to maximize use of tensor cores, achieving better real-world performance despite higher theoretical FLOP counts. This performance analysis illustrates a crucial principle in modern hardware-efficient deep learning: raw FLOP counts don’t always translate directly to wall-clock time. The ability to leverage specialized hardware accelerators and maintain high GPU utilization often matters more than theoretical operation counts. Our chunkwise implementation succeeds by aligning the computation with these hardware realities.</p> <p>Finally, we compare DeltaNet’s training throughput against other models at the 1.3B parameter scale.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/throughputs.png" alt="示例图片" style="width: 75%"> </div> </div> <p>DeltaNet achieves competitive throughput, running only slightly slower than GLA (Gated Linear Attention). This small performance gap is a reasonable trade-off for DeltaNet’s more expressive transition matrices.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-03-delta.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Songlin Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>